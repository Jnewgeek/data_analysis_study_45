如果一个人在百分制的考试中得了 95 分，你肯定会认为他学习成绩很好，如果得了 65 分，就会觉得他成绩不好。如果得了 80 分呢？你会觉得他成绩中等，因为在班级里这属于大部分人的情况。因为我们从小到大的考试成绩基本上都会满足正态分布的情况。以下图为例，在正态分布中，大部分人的成绩会集中在中间的区域，少部分人处于两头的位置。
<center class="half">
    <img src="..\img\13讲_正态分布.jpg" width="350"/>
</center>
<center>正态分布</center>



### 数据变换在数据分析中的角色
在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。
<center class="half">
    <img src="..\img\13讲_数据变换流程.jpg" width="500"/>
</center>

从整个流程中可以看出，数据变换是数据准备的重要环节，它通过**数据平滑、数据聚集、数据概化和规范化**等方式将数据转换成适用于数据挖掘的形式。

常见的变换方法：
- `数据平滑`：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式；
- `数据聚集`：对数据进行汇总，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和；
- `数据概化`：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国。
- `数据规范化`：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等；
- `属性构造`：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。

### 数据规范化的几种方法

**1. Min-max 规范化**

Min-max 规范化方法是将原始数据变换到[0,1]的空间中。用公式表示就是：
$$
X_{new} = \frac{X-X_{min}}{X_{max}-X_{min}}
$$

**2. Z-Score 规范化**

$$
\mu = \frac{1}{n}\sum^{N}_{i=1}{X_{i}}
$$

$$
\sigma = \sqrt{\frac{1}{N}\sum^{N}_{i=1}{(X_{i}-\mu)^2}}
$$

$$
X_{new} = \frac{X - \mu}{\sigma}
$$
Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。

**3. 小数定标规范化**

小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A 的取值中的最大绝对值。

### Python 的 SciKit-Learn 库使用

SciKit-Learn 是 Python 的重要机器学习库，它帮我们封装了大量的机器学习算法，比如分类、聚类、回归、降维等。此外，它还包括了数据变换模块。

**1. Min-max 规范化**

我们可以让原始数据投射到指定的空间[min, max]，在 **SciKit-Learn** 里有个函数 `MinMaxScaler` 是专门做这个的，它允许我们给定一个最大值与最小值，然后将原数据投射到[min, max]中。


```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np
# 初始化数据，每一行表示一个样本，每一列表示一个特征
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
min_max_scaler = MinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print(minmax_x)
```

    [[0.         0.         0.66666667]
     [1.         1.         1.        ]
     [0.         1.         0.        ]]


**1.1 根据原理实现**

根据**min-max**原理手动实现数据归一化，并采用同样的函数调用接口。


```python
class MyMinMaxScaler:
    
    def __init__(self):
        self.min = None
        self.max = None
        
    def fit(self, x):
        x = np.array(x)
        self.min = x.min(axis = 0)
        self.max = x.max(axis = 0)
        
    def transform(self, x):
        return (x - self.min) / (self.max - self.min)
    
    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)
    
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
min_max_scaler = MyMinMaxScaler()
minmax_x = min_max_scaler.fit_transform(x)
print(minmax_x)
```

    [[0.         0.         0.66666667]
     [1.         1.         1.        ]
     [0.         1.         0.        ]]


**2. Z-Score 规范化**

在 **SciKit-Learn** 库中使用函数，`StandardScaler`可以直接将给定数据进行 **Z-Score** 规范化。


```python
from sklearn.preprocessing import StandardScaler

x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
z_scaler  = StandardScaler()
scaled_x  = z_scaler .fit_transform(x)
print(scaled_x)
```

    [[-0.70710678 -1.41421356  0.26726124]
     [ 1.41421356  0.70710678  1.06904497]
     [-0.70710678  0.70710678 -1.33630621]]


**2.1 根据原理实现**

根据**Z-Score**原理手动实现数据归一化，并采用同样的函数调用接口。


```python
class MyStandardScaler:
    
    def __init__(self):
        self.mean = None
        self.std = None
    
    def fit(self, x):
        x = np.array(x)
        self.mean = x.mean(axis=0)
        self.std = x.std(axis=0)
        
    def transform(self, x):
        return (x - self.mean) / self.std
    
    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)
    
x = np.array([[ 0., -3.,  1.],
              [ 3.,  1.,  2.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
z_scaler  = MyStandardScaler()
scaled_x  = z_scaler .fit_transform(x)
print(scaled_x)
```

    [[-0.70710678 -1.41421356  0.26726124]
     [ 1.41421356  0.70710678  1.06904497]
     [-0.70710678  0.70710678 -1.33630621]]


**3. 小数定标规范化**

在 SciKit-Learn 库中没有直接可用的库，我们需要用 NumPy 库来计算小数点的位数。


```python
class MyFloatScaler:
    
    def __init__(self):
        self.digit = None
        
    def fit(self, x):
        x = np.array(x)
        self.digit = np.ceil(np.log10(np.max(abs(x), axis = 0)))
        
    def transform(self, x):
        return x / 10 ** self.digit
    
    def fit_transform(self, x):
        self.fit(x)
        return self.transform(x)
    
x = np.array([[ 1., -300.,  1.],
              [ 30.,  1.,  20.],
              [ 0.,  1., -1.]])
# 将数据进行[0,1]规范化
f_scaler  = MyFloatScaler()
scaled_x  = f_scaler .fit_transform(x)
print(scaled_x)
```

    [[ 0.01  -0.3    0.01 ]
     [ 0.3    0.001  0.2  ]
     [ 0.     0.001 -0.01 ]]


### 数据挖掘中数据变换比算法选择更重要

在数据变换中，重点是如何将数值进行规范化，有三种常用的规范方法，分别是 Min-Max 规范化、Z-Score 规范化、小数定标规范化。其中 Z-Score 规范化可以直接将数据转化为正态分布的情况，当然不是所有自然界的数据都需要正态分布，我们也可以根据实际的情况进行设计，比如取对数 log，或者神经网络里采用的激励函数等。

<center class="half">
    <img src="..\img\13讲_总结.jpg" width="500"/>
</center>


参考资料：

[极客时间-数据分析实战45讲](https://time.geekbang.org/column/intro/147)
